# TiniestStoryTeller

TiniestStoryTeller aims to demonstrate that efficient language learning models (LLM) can be developed and trained under significant resource constraints. This project is designed to run on standard desktop PCs or laptops, making it accessible for researchers and enthusiasts without access to high-end computational resources. Our initiative serves as a counterexample to the current trend of ever-larger models requiring substantial computational power.


## Features

- Custom LLM Training
- Data Sampling 

