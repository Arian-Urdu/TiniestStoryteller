from openai import OpenAI
import numpy as np
import os
from load_checkpoint import model
from config import tokenizer, device
import random


client = OpenAI(
    base_url="http://localhost:11434/v1",
    api_key = "ollama",
)

system_prompt = ("You will be given a prompt, an expected response, and a response generated by a language model. "
                 "Your task is to evaluate how logically consistent and appropriate the generated response is "
                 "compared to the expected response. "
                 "Please provide a score between 1.0 and 10.0 where 10.0 is the highest. "
                 "Output nothing but the score, no additional text or explanation.\n"
                 "Prompt: {prompt}\n"
                 "Example response: {example_response}\n"
                 "Generated response: {llm_response}\n")


number_prompts = 10

scores = []

indices = random.sample(range(1, number_prompts + 1), number_prompts)

prompts_path = os.path.join(os.path.dirname(os.path.realpath(__file__)), "prompts_for_logic_eval.txt")
with open(prompts_path, 'r', encoding='utf-8') as f:
    prompts = f.read().split('\n')

for i in indices:
    # get random prompt from file
    prompt, example_response = prompts[i].split("---")

    # let model generate
    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)
    model_response = tokenizer.decode(model.generate(input_ids, max_new_tokens=10)[0].tolist())

    # remove prompt from response
    model_response = model_response[len(prompt) + 1:]
    # cut off after first full stop
    model_response = model_response.split(".")[0] + "."

    # prepare prompt
    system_prompt = system_prompt.format(prompt=prompt, example_response=example_response, llm_response=model_response)
    print(f"Prompt: {system_prompt}")
    msgs = [
        {"role": "system", "content": system_prompt},
    ]

    # let llm score output
    score = client.chat.completions.create(model="llama3", messages=msgs).choices[0].message.content
    print(f"Score: {score}")
    if float(score) < 1.0 or float(score) > 10:
        raise ValueError(f"Score must be between 1.0 and 10.0. Got {score}")
    scores.append(float(score))
    print("\n-------------------------------\n")

print(f"Final score: {np.mean(scores)}")
