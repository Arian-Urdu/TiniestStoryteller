import torch
from openai import OpenAI
import numpy as np
import os
from evaluation.load_checkpoint import load_model
from config import tokenizer, device
import random


client = OpenAI(
    base_url="http://localhost:11434/v1",
    api_key = "ollama",
)




number_prompts = 10

scores = []

indices = random.sample(range(1, number_prompts + 1), number_prompts)

prompts_path = os.path.join(os.path.dirname(os.path.realpath(__file__)), "prompts_for_logic_eval.txt")
with open(prompts_path, 'r', encoding='utf-8') as f:
    prompts = f.read().split('\n')


def evaluate_model_logic(model = None, modelpath = ""):
    system_prompt = ("You will be given a prompt, an expected response, and a response generated by a language model. "
                     "Your task is to evaluate how logically consistent and appropriate the generated response is "
                     "compared to the expected response. Please provide a score between 1 and 3: "
                     "1 is bad, 2 okay, and 3 good. "
                     "Don't output any numbers below 1 or greater than 3! "
                     "Output nothing but the score, no additional text or explanation.\n"
                     "Prompt: {prompt}\n"
                     "Example response: {example_response}\n"
                     "Generated response: {llm_response}\n")

    if model == None:
        if modelpath == "":
            raise Exception("Invalid arguments")
        model = load_model(modelpath)

    for i in indices:
        # get random prompt from file
        prompt, example_response = prompts[i].split("---")

        # let model generate
        input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)
        with torch.no_grad():
            model_response = model.generate(input_ids, max_new_tokens=10)
        model_response = tokenizer.decode(model_response[0].tolist())

        # remove prompt from response
        model_response = model_response[len(prompt) + 1:]
        # cut off after first full stop
        model_response = model_response.split(".")[0] + "."

        # prepare prompt
        system_prompt = system_prompt.format(prompt=prompt, example_response=example_response, llm_response=model_response)
        msgs = [
            {"role": "system", "content": system_prompt},
        ]

        # let llm score output
        score = float(client.chat.completions.create(model="llama3", messages=msgs).choices[0].message.content)
        if score < 1.0:
            score = 1.0
        elif score > 3.0:
            score = 3.0
        scores.append(score)

    return np.mean(scores)
